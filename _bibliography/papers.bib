---
---

@article{tuli2022flexibert,
 abstract = {The existence of a plethora of language models makes the problem of selecting the best one for a custom task challenging. Most state-of-the-art methods leverage transformer-based models (e.g., BERT) or their variants. Training such models and exploring their hyperparameter space, however, is computationally expensive. Prior work proposes several neural architecture search (NAS) methods that employ performance predictors (e.g., surrogate models) to address this issue; however, analysis has been limited to homogeneous models that use fixed dimensionality throughout the network. This leads to sub-optimal architectures. To address this limitation, we propose a suite of heterogeneous and flexible models, namely FlexiBERT, that have varied encoder layers with a diverse set of possible operations and different hidden dimensions. For better-posed surrogate modeling in this expanded design space, we propose a new graph-similarity-based embedding scheme. We also propose a novel NAS policy, called BOSHNAS, that leverages this new scheme, Bayesian modeling, and second-order optimization, to quickly train and use a neural surrogate model to converge to the optimal architecture. A comprehensive set of experiments shows that the proposed policy, when applied to the FlexiBERT design space, pushes the performance frontier upwards compared to traditional models. FlexiBERT-Mini, one of our proposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9% higher GLUE score. A FlexiBERT model with equivalent performance as the best homogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed model, achieves state-of-the-art results, outperforming the baseline models by at least 5.7% on the GLUE benchmark.},
 arxiv = {2205.11656},
 author = {Tuli, Shikhar and Dedhia, Bhishma and Tuli, Shreshth and Jha, Niraj K.},
 citationlist = {#},
 citations = {0},
 selected = {true},
 title = {FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?},
 year = {2022}
}

@article{tuli2021generative,
 abstract = {In standard generative deep learning models, such as autoencoders or GANs, the size of the parameter set is proportional to the complexity of the generated data distribution. A significant challenge is to deploy resource-hungry deep learning models in devices with limited memory to prevent system upgrade costs. To combat this, we propose a novel framework called generative optimization networks (GON) that is similar to GANs, but does not use a generator, significantly reducing its memory footprint. GONs use a single discriminator network and run optimization in the input space to generate new data samples, achieving an effective compromise between training time and memory consumption. GONs are most suited for data generation problems in limited memory settings. Here we illustrate their use for the problem of anomaly detection in memory-constrained edge devices arising from attacks or intrusion events. Specifically, we use a GON to calculate a reconstruction-based anomaly score for input time-series windows. Experiments on a Raspberry-Pi testbed with two existing and a new suite of datasets show that our framework gives up to 32% higher detection F1 scores and 58% lower memory consumption, with only 5% higher training overheads compared to the state-of-the-art.},
 arxiv = {2110.02912},
 author = {Tuli, Shreshth and Tuli, Shikhar and Casale, Giuliano and Jennings, Nicholas R},
 citationlist = {http://scholar.google.com/scholar?cites=15582971490951281448&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {3},
 code = {https://github.com/imperial-qore/GON},
 journal = {NeurIPS 2021 - Workshop on ML for Systems},
 selected = {true},
 title = {Generative Optimization Networks for Memory Efficient Data Generation},
 year = {2021}
}

@article{tuli2021are,
 abstract = {Modern machine learning models for computer vision exceed humans in accuracy on specific visual recognition tasks, notably on datasets like ImageNet. However, high accuracy can be achieved in many ways. The particular decision function found by a machine learning system is determined not only by the data to which the system is exposed, but also the inductive biases of the model, which are typically harder to characterize. In this work, we follow a recent trend of in-depth behavioral analyses of neural network models that go beyond accuracy as an evaluation metric by looking at patterns of errors. Our focus is on comparing a suite of standard Convolutional Neural Networks (CNNs) and a recently-proposed attention-based network, the Vision Transformer (ViT), which relaxes the translation-invariance constraint of CNNs and therefore represents a model with a weaker set of inductive biases. Attention-based networks have previously been shown to achieve higher accuracy than CNNs on vision tasks, and we demonstrate, using new metrics for examining error consistency with more granularity, that their errors are also more consistent with those of humans. These results have implications both for building more human-like vision models, as well as for understanding visual object recognition in humans.},
 arxiv = {2105.07197},
 author = {Tuli, Shikhar and Dasgupta, Ishita and Grant, Erin and Griffiths, Thomas L.},
 citationlist = {http://scholar.google.com/scholar?cites=667529829221842193&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {39},
 code = {https://github.com/shikhartuli/cnn_txf_bias},
 journal = {Annual Meeting of the Cognitive Science Society (CogSci)},
 selected = {true},
 title = {Are Convolutional Neural Networks or Transformers more like human vision?},
 year = {2021}
}

@article{tuli2020avac,
 arxiv = {2005.03077},
 author = {Tuli, Shikhar and Tuli, Shreshth},
 citationlist = {http://scholar.google.com/scholar?cites=2777597957252907747&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {2},
 journal = {IEEE International Symposium on Circuits and Systems (ISCAS)},
 selected = {true},
 title = {AVAC: A Machine Learning based Adaptive RRAM Variability-Aware Controller for Edge Devices},
 year = {2020}
}

@inproceedings{tuli2020rram,
 author = {Tuli, Shikhar and Rios, Marco and Levisse, Alexandre and Atienza, David},
 citationlist = {https://scholar.google.com/scholar?cites=7027936382486962646&as_sdt=5,31&sciodt=0,31&hl=en},
 citations = {7},
 code = {https://github.com/shikhartuli/rram-vac},
 journal = {25th Asia and South Pacific Design Automation Conference (ASPDAC)},
 pages = {181--186},
 selected = {true},
 title = {RRAM-VAC: A variability-aware controller for {RRAM}-based memory architectures},
 year = {2020}
}

@article{tuli2020design,
 articleno = {14},
 author = {Tuli, Shikhar and Bhowmik, Debanjan},
 citationlist = {https://scholar.google.com/scholar?cites=10186738064493107792&as_sdt=5,31&sciodt=0,31&hl=en},
 citations = {1},
 journal = {International Conference on Neuromorphic Systems (ICONS)},
 numpages = {8},
 selected = {true},
 title = {Design of a Conventional-Transistor-Based Analog Integrated Circuit for On-Chip Learning in a Spiking Neural Network},
 year = {2020}
}

@article{tuli2019fogbus,
 arxiv = {1811.11978},
 author = {Tuli, Shreshth and Mahmud, Redowan and Tuli, Shikhar and Buyya, Rajkumar},
 citationlist = {http://scholar.google.com/scholar?cites=17493334207794570486&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {224},
 code = {https://github.com/Cloudslab/FogBus},
 journal = {Journal of Systems and Software},
 pages = {22--36},
 pdf = {fogbus.pdf},
 publisher = {Elsevier},
 supp = {https://github.com/Cloudslab/FogBus/blob/master/Manuals/End-user-tutorial/fogbus-end-user.pdf},
 title = {Fogbus: A blockchain-based lightweight framework for edge and fog computing},
 volume = {154},
 year = {2019}
}

@article{gupta2020characterization,
 author = {Gupta, Charu and Gupta, Anshul and Tuli, Shikhar and Bury, Erik and Parvais, Bertrand and Dixit, Abhisek},
 citationlist = {http://scholar.google.com/scholar?cites=8158582607463640183&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {2},
 journal = {IEEE Transactions on Electron Devices},
 number = {1},
 pages = {4-10},
 selected = {true},
 title = {Characterization and Modeling of Hot Carrier Degradation in N-Channel Gate-All-Around Nanowire FETs},
 volume = {67},
 year = {2020}
}

@article{jindal2019dhoom,
 author = {Jindal, Neetu and Chandran, Sandeep and Panda, Preeti Ranjan and Prasad, Sanjiva and Mitra, Abhay and Singhal, Kunal and Gupta, Shubham and Tuli, Shikhar},
 citationlist = {http://scholar.google.com/scholar?cites=6988318319033446326&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {9},
 journal = {56th ACM/IEEE Design Automation Conference (DAC)},
 pages = {1-6},
 title = {DHOOM: Reusing Design-for-Debug Hardware for Online Monitoring},
 year = {2019}
}

@article{gill2019transformative,
 arxiv = {1911.01941},
 author = {Gill, Sukhpal Singh and Tuli, Shreshth and Xu, Minxian and Singh, Inderpreet and Singh, Karan Vijay and Lindsay, Dominic and Tuli, Shikhar and Smirnova, Daria and Singh, Manmeet and Jain, Udit and others},
 citationlist = {http://scholar.google.com/scholar?cites=3818194902295987144&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {212},
 journal = {Internet of Things},
 pages = {100118},
 pdf = {transformative.pdf},
 publisher = {Elsevier},
 title = {Transformative effects of IoT, Blockchain and Artificial Intelligence on cloud computing: Evolution, vision, trends and open challenges},
 volume = {8},
 year = {2019}
}

@article{tuli2020modelling,
 author = {Tuli, Shreshth and Tuli, Shikhar and Verma, Ruchi and Tuli, Rakesh},
 citationlist = {http://scholar.google.com/scholar?cites=10793541038144279309&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {12},
 code = {https://github.com/shreshthtuli/covid-19-prediction},
 journal = {Biomedical Research and Clinical Reviews},
 medrxiv = {10.1101/2020.06.18.20134874v1},
 publisher = {Cold Spring Harbor Laboratory Press},
 supp = {https://www.medrxiv.org/content/10.1101/2020.06.18.20134874v1.supplementary-material},
 title = {Modelling for prediction of the spread and severity of COVID-19 and its association with socioeconomic factors and virus types},
 year = {2020}
}

@article{tuli2020predicting,
 author = {Tuli, Shreshth and Tuli, Shikhar and Tuli, Rakesh and Gill, Sukhpal Singh},
 citationlist = {http://scholar.google.com/scholar?cites=832410515520312149&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {337},
 code = {https://github.com/shreshthtuli/covid-19-prediction},
 journal = {Internet of Things},
 medrxiv = {10.1101/2020.05.06.20091900v1},
 pages = {100222},
 publisher = {Elsevier},
 selected = {true},
 title = {Predicting the Growth and Trend of COVID-19 Pandemic using Machine Learning and Cloud Computing},
 year = {2020}
}

@article{tuli2020next,
 author = {Tuli, Shreshth and Tuli, Shikhar and Wander, Gurleen and Wander, Praneet and Gill, Sukhpal Singh and Dustdar, Schahram and Sakellariou, Rizos and Rana, Omer},
 citationlist = {http://scholar.google.com/scholar?cites=7608096699247473736&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {52},
 journal = {Internet Technology Letters},
 number = {2},
 pages = {e145},
 pdf = {https://onlinelibrary.wiley.com/doi/epdf/10.1002/itl2.145},
 publisher = {Wiley Online Library},
 title = {Next generation technologies for smart healthcare: Challenges, vision, model, trends and future directions},
 volume = {3},
 year = {2020}
}

@article{tuli2019apex,
 arxiv = {1910.01642},
 author = {Tuli, Shreshth and Tuli, Shikhar and Jain, Udit and Buyya, Rajkumar},
 citationlist = {http://scholar.google.com/scholar?cites=9962629353040534727&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {1},
 code = {https://github.com/HS-Optimization-with-AI/Apex},
 journal = {11th IEEE International Conference on Cloud Computing},
 title = {APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices},
 year = {2019}
}

@article{tuli2019fogbus,
 arxiv = {1811.11978},
 author = {Tuli, Shreshth and Mahmud, Redowan and Tuli, Shikhar and Buyya, Rajkumar},
 citationlist = {http://scholar.google.com/scholar?cites=17493334207794570486&as_sdt=2005&sciodt=0,5&hl=en},
 citations = {224},
 code = {https://github.com/Cloudslab/FogBus},
 journal = {Journal of Systems and Software},
 pages = {22--36},
 pdf = {fogbus.pdf},
 publisher = {Elsevier},
 supp = {https://github.com/Cloudslab/FogBus/blob/master/Manuals/End-user-tutorial/fogbus-end-user.pdf},
 title = {Fogbus: A blockchain-based lightweight framework for edge and fog computing},
 volume = {154},
 year = {2019}
}

